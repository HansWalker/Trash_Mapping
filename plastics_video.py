# -*- coding: utf-8 -*-
"""Plastics_video.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10zXBSatIRo9yjSmDDc58nD1kahowdCLz

# Install detectron2
"""

!pip install pyyaml

!pip install torch torchvision -f https://download.pytorch.org/whl/torch_stable.html

import torch
print("PyTorch version:", torch.__version__)
print("CUDA version:", torch.version.cuda)

!git clone https://github.com/facebookresearch/detectron2.git
!pip install -e detectron2

!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.11/index.html

# Check PyTorch installation
import torch, torchvision

# Print the installed version and check CUDA availability
print(f"PyTorch version: {torch.__version__}, CUDA available: {torch.cuda.is_available()}")

# Check if the installed PyTorch version starts with a specific string, e.g., "1.9" or "2.1"
target_versions = ["1.9", "2.1"]
if not any(torch.__version__.startswith(ver) for ver in target_versions):
    print(f"Please manually install one of the target PyTorch versions: {target_versions}")

!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# Import some common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

# Import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

"""# Run a pre-trained detectron2 model

We first download an image from the COCO dataset:
"""

# Download the image using wget
!wget https://as1.ftcdn.net/v2/jpg/06/20/75/16/1000_F_620751695_UaJ8sY7P5C5QKMKTFotiXhh5K4pgflNA.jpg -O downloaded_image.jpg

# Import necessary libraries
import cv2
import matplotlib.pyplot as plt

# Read the downloaded image from disk
image = cv2.imread('downloaded_image.jpg')

# Convert the image from BGR to RGB
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Display the image using matplotlib
plt.imshow(image_rgb)
plt.axis('off')  # Turn off axis numbers and ticks
plt.show()

"""Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."""

cfg = get_cfg()

"""### Object Detection
["...object detection, where the goal is to classify individual objects and localize them using a bounding box..."](https://kharshit.github.io/blog/2019/08/23/quick-intro-to-instance-segmentation)
"""

# Install dependencies
!pip install -U torch torchvision
!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html

# Download the image
!wget "https://as1.ftcdn.net/v2/jpg/06/20/75/16/1000_F_620751695_UaJ8sY7P5C5QKMKTFotiXhh5K4pgflNA.jpg" -O "downloaded_image.jpg"

# Import necessary libraries
import cv2
import matplotlib.pyplot as plt
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor

# Setup detectron2 model
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)

# Read the image
im = cv2.imread("downloaded_image.jpg")

# Make prediction
outputs = predictor(im)

# Import visualization utils
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

# Visualize the result
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
plt.imshow(v.get_image()[:, :, ::-1])
plt.axis('off')
plt.show()

# Object Detection
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)

# Object Detection Visualizer
predictions = predictor(im)["instances"]
# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(predictions.to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

"""### Instance Segmentation
["...instance segmentation, we care about detection and segmentation of the instances of objects separately"](https://kharshit.github.io/blog/2019/08/23/quick-intro-to-instance-segmentation)

In other words, we perform segmentation only on the objects detected within the bounding box of object detection.
"""

# Instance Segmentation Visualizer
predictions = predictor(im)["instances"]
# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(predictions.to("cpu"))
cv2_imshow(out.get_image()[:, :, ::-1])

"""### Panoptic Segmentation
["...panoptic segmentation combines semantic and instance segmentation such that all pixels are assigned a class label and all object instances are uniquely segmented."](https://kharshit.github.io/blog/2019/10/18/introduction-to-panoptic-segmentation-tutorial)

Panoptic segmentation classifies all pixels in the image within a polygonal bounding area including objects and background scenery. Unlike, object and Instance segmentation which only care about individual objects in the image.


"""

# Panoptic Segmentation
# Ref: https://www.youtube.com/watch?v=ju_2NuK5O-E
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor = DefaultPredictor(cfg)

# Panoptic Segmentation Visualizer
# We can use `Visualizer` to draw the predictions on the image.
predictions, segmentInfo = predictor(im)["panoptic_seg"]
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1)
# Uncomment to filter out specific segments
# out = v.draw_panoptic_seg_predictions(predictions.to("cpu"), list(filter(lambda x: x['category_id'] == 17, segmentInfo)), area_threshold=.1)
out = v.draw_panoptic_seg_predictions(predictions.to("cpu"), segmentInfo, area_threshold=.1)
cv2_imshow(out.get_image()[:, :, ::-1])

"""### Process Video Panoptic Segmentation

Ref: https://www.geeksforgeeks.org/python-opencv-capture-video-from-camera/

[Sample video](https://www.istockphoto.com/video/forward-driving-perspective-on-pennsylvania-avenue-in-dc-gm911535028-250974474)
"""

# Load video sample
!wget https://www.youtube.com/watch?v=ju_2NuK5O-E -q -O dc-street.mp4

!pip install pytube

import urllib.request
import cv2
import os
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

# Create folder to save frames
if not os.path.exists('segmented_frames'):
    os.makedirs('segmented_frames')

try:
    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
    predictor = DefaultPredictor(cfg)
except Exception as e:
    print(f"Failed to initialize Detectron2: {e}")
    exit(1)

try:
    video_url = 'https://v.ftcdn.net/03/31/78/56/700_F_331785611_2UwlQOP7rZByFjNy2FHdhHgyBKe2vpjo_ST.mp4'
    urllib.request.urlretrieve(video_url, 'downloaded_video.mp4')
except Exception as e:
    print(f"Failed to download video: {e}")
    exit(1)

try:
    vid = cv2.VideoCapture('downloaded_video.mp4')
    if not vid.isOpened():
        raise Exception("Could not open video")
except Exception as e:
    print(f"Failed to initialize video capture: {e}")
    exit(1)

frame_count = 0
while True:
    ret, frame = vid.read()
    frame_count += 1

    if not ret:
        print("End of video or cannot read frame.")
        break

    try:
        outputs = predictor(frame)
    except Exception as e:
        print(f"Prediction Error: {e}")
        continue

    if "panoptic_seg" in outputs:
        panoptic_seg, segments_info = outputs["panoptic_seg"]

        try:
            v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
            out = v.draw_panoptic_seg_predictions(panoptic_seg.to("cpu"), segments_info)
            segmented_image = out.get_image()[:, :, ::-1]

            # Save the segmented frame to the folder
            frame_filename = os.path.join('segmented_frames', f'frame_{frame_count}.png')
            cv2.imwrite(frame_filename, segmented_image)

        except Exception as e:
            print(f"Visualization Error: {e}")
            continue

print("Releasing resources...")
vid.release()
cv2.destroyAllWindows()

!pip install opencv-python

from IPython.display import HTML
from base64 import b64encode

video_path = "/content/processed_video.mp4"

mp4 = open(video_path, 'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

HTML("""
<video width=400 controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)